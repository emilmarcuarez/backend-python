# services/malware.py  — endurecido
import os, re, time, base64, math, json, xml.etree.ElementTree as ET
from urllib.parse import urljoin, urlparse
from typing import List, Tuple, Dict

import requests
from bs4 import BeautifulSoup

# Playwright opcional (degrada a requests si falla)
try:
    from playwright.sync_api import sync_playwright
    _HAS_PW = True
except Exception:
    _HAS_PW = False

UA = {"User-Agent": "IDEI-Auditor/1.1 (+analisis.ideidev.com)"}
TIMEOUT = (8, 25)

# ================= Heurísticas ampliadas =================

# Palabras clave ES/EN: gambling, pharma, adult, crypto, loans, piratería, etc.
SUS_KEYWORDS = [
    # Gambling (ES/EN)
    r"\bcasino(s)?\b", r"\bruleta\b", r"\btragamonedas\b", r"\btragaperras\b",
    r"\bslots?\b", r"\bblackjack\b", r"\bbacara?t\b", r"\bapuestas?\b",
    r"\bbono(s)?\b", r"\bgiros\b", r"\bfree\s*spins?\b",
    # Pharma
    r"\bviagra\b", r"\bcialis\b", r"\blevitra\b", r"\bphentermine\b",
    r"\bpildoras?\b", r"\bpastillas?\b", r"\bpoten(ci|cy)\b",
    # Adult
    r"\bporno?\b", r"\bxxx\b", r"\berotic\b", r"\bescort(s)?\b",
    # Crypto/Scam
    r"\bcrypto\b", r"\bbitcoin\b", r"\beth(ereum)?\b", r"\bforex\b",
    r"\binvest(ment)?\b", r"\bprofit\b", r"\breturns?\b",
    # Otros spam SEO comunes
    r"\bdescargas? (gratis|free)\b", r"\bserial(es)?\b",
    r"\bcrack(eado)?\b", r"\bkeygen\b", r"\blicencia gratis\b",
]

# Firmas populares de JS malicioso/obfuscado
SIGNS_JS = [
    r"eval\s*\(", r"Function\s*\(", r"document\.write\s*\(", r"atob\s*\(",
    r"unescape\s*\(", r"fromCharCode\s*\(", r"setTimeout\s*\(.{0,120}eval\s*\(",
    r"eval\(function\(p,a,c,k,e,d\)",  # Packer clásico
    r"new Function\s*\(",
    r"window\.onload\s*=",
    r"\.replace\s*\(.{0,80}/.*\/[gimuy]{0,5}\)",  # replaces con regex sospechosos
    r"\\x[0-9a-fA-F]{2}",  # hex escapado masivo
]

# Estilos / artefactos ocultos
HIDDEN_STYLE = [
    r"display\s*:\s*none", r"visibility\s*:\s*hidden",
    r"opacity\s*:\s*0(?:\.0+)?\b", r"font-size\s*:\s*0\b",
    r"position\s*:\s*absolute;?\s*left\s*:\s*-\d+px",  # fuera de viewport
]

# Dominios/patrones externos a vigilar (denylist corta — evita falsos positivos)
SUS_EXTERNAL_PATTERNS = [
    r"(?:fast|best|super|ultra)[-]?(?:cdn|js|script)\d*\.",
    r"-(?:cdn|js)\.top\b",
    r"\.(?:xyz|top|gq|cn|ru)(?:/|$)",
    r"(?:bet|casino|slot)s?\.",
]

# iFrames y meta refresh
IFRAME_SUS = [
    r"<iframe[^>]+width\s*=\s*['\"]?1['\"]?[^>]*height\s*=\s*['\"]?1['\"]?[^>]*>",
    r"<iframe[^>]+style=['\"][^>]*display\s*:\s*none[^>]*['\"][^>]*>",
]
META_REFRESH = r'<meta[^>]+http-equiv=["\']refresh["\'][^>]+content=["\']\s*\d+\s*;\s*url='

# ================= Utilidades =================

def same_host(u0, u1):
    try:
        h0 = (urlparse(u0).hostname or "").lower()
        h1 = (urlparse(u1).hostname or "").lower()
        return h0 and h1 and h0 == h1
    except Exception:
        return False

def _fetch(url) -> str:
    try:
        r = requests.get(url, headers=UA, timeout=TIMEOUT, allow_redirects=True)
        if r.status_code == 200:
            return r.text or ""
    except Exception:
        pass
    return ""

def entropy(s: str) -> float:
    # Entropía Shannon simple sobre bytes
    if not s:
        return 0.0
    import collections, math
    counts = collections.Counter(s)
    l = float(len(s))
    return -sum((c/l) * math.log2(c/l) for c in counts.values())

def collect_links(base_url, html, limit=120):
    out = []
    soup = BeautifulSoup(html or "", "lxml")
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        if href.startswith("#"):
            continue
        url = urljoin(base_url, href)
        if same_host(base_url, url):
            out.append(url.split("#")[0])
    # prioriza sospechosas por keywords
    def score(u: str):
        s = 0
        for kw in SUS_KEYWORDS:
            if re.search(kw, u, flags=re.I):
                s += 5
        return -s, len(u)
    out = sorted(list(dict.fromkeys(out)))
    out = sorted(out, key=score)
    return out[:limit]

def parse_sitemap(xml_text: str) -> List[str]:
    out = []
    if not xml_text:
        return out
    try:
        tree = ET.fromstring(xml_text)
        ns = {"sm": "http://www.sitemaps.org/schemas/sitemap/0.9"}
        for e in tree.findall(".//sm:loc", ns):
            if e.text:
                out.append(e.text.strip())
        return out
    except Exception:
        # Fallback por regex
        locs = re.findall(r"<loc>(.*?)</loc>", xml_text, re.I)
        return [l.strip() for l in locs]

def discover_urls(base_url, limit=500) -> List[str]:
    """
    Descubre URLs con sitemap(s) (wp-sitemap.xml, sitemap.xml, sitemap_index.xml),
    y también usando WP REST Search para términos “calientes”.
    """
    urls = []
    bases = [
        "/wp-sitemap.xml",          # WP core
        "/sitemap.xml",             # genérico
        "/sitemap_index.xml",       # Yoast / RankMath
        "/post-sitemap.xml",        # sitemaps hijos
        "/page-sitemap.xml",
        "/category-sitemap.xml",
    ]
    host = urlparse(base_url).scheme + "://" + (urlparse(base_url).hostname or "")
    seen = set()
    for b in bases:
        xml = _fetch(urljoin(host, b))
        for u in parse_sitemap(xml):
            if same_host(base_url, u) and u not in seen:
                seen.add(u)
                urls.append(u)

    # REST Search de WP para múltiples términos (reduce ruido)
    try:
        terms = ["casino", "slot", "ruleta", "apuesta", "viagra", "cialis", "xxx", "crypto", "bitcoin", "forex", "descarga", "crack"]
        for t in terms:
            q = requests.get(urljoin(base_url, "/wp-json/wp/v2/search"),
                             params={"search": t, "per_page": 50},
                             headers=UA, timeout=(6, 12))
            if q.status_code == 200:
                for item in q.json():
                    link = item.get("url") or item.get("link")
                    if link and same_host(base_url, link) and link not in seen:
                        seen.add(link); urls.append(link)
    except Exception:
        pass

    return urls[:limit]

def render_dom(url) -> Tuple[str, str, List[str]]:
    """
    Devuelve (html_raw, html_dom, scripts_externos)
    """
    html_raw, html_dom, externals = "", "", []
    try:
        r = requests.get(url, headers=UA, timeout=TIMEOUT)
        html_raw = r.text or ""
    except Exception:
        pass

    if _HAS_PW:
        try:
            with sync_playwright() as p:
                b = p.chromium.launch(headless=True, args=["--no-sandbox"])
                c = b.new_context(ignore_https_errors=True, user_agent=UA["User-Agent"])
                page = c.new_page()
                page.goto(url, wait_until="networkidle", timeout=35000)
                # Espera breve adicional para late-injections
                page.wait_for_timeout(500)
                html_dom = page.content() or ""
                externals = page.evaluate("""() => Array.from(document.scripts)
                    .filter(s => s.src).map(s => s.src)""") or []
                b.close()
        except Exception:
            # degrada
            pass

    return html_raw, html_dom, externals

def decode_base64_chunks(text: str) -> List[str]:
    samples = []
    for m in re.finditer(r"(?<![A-Za-z0-9+/=])([A-Za-z0-9+/]{100,}={0,2})(?![A-Za-z0-9+/=])", text):
        blob = m.group(1)
        try:
            dec = base64.b64decode(blob + "==", validate=False)
            if dec and len(dec) >= 80:
                samples.append(dec.decode("utf-8", "ignore")[:800])
        except Exception:
            pass
        if len(samples) >= 5:
            break
    return samples

def find_indicators(html_text: str) -> Tuple[List[str], List[str]]:
    findings = []
    text = html_text or ""

    # JS malicioso/obfuscado
    for sig in SIGNS_JS:
        if re.search(sig, text, flags=re.I | re.S):
            findings.append(f"js:{sig}")

    # CSS oculto
    for hs in HIDDEN_STYLE:
        if re.search(hs, text, flags=re.I):
            findings.append(f"hidden:{hs}")

    # Keywords
    kw_hits = 0
    for kw in SUS_KEYWORDS:
        if re.search(kw, text, flags=re.I):
            kw_hits += 1
    if kw_hits >= 2:
        findings.append(f"keywords:{kw_hits}")

    # iFrames y meta refresh
    for fr in IFRAME_SUS:
        if re.search(fr, text, flags=re.I):
            findings.append("iframe:hidden")
            break
    if re.search(META_REFRESH, text, flags=re.I):
        findings.append("meta:refresh")

    # Externals raros
    ext_sus = 0
    for pat in SUS_EXTERNAL_PATTERNS:
        if re.search(pat, text, flags=re.I):
            ext_sus += 1
    if ext_sus:
        findings.append(f"externals:{ext_sus}")

    # Entropía alta en scripts grandes (trozos > 600 chars)
    chunks = re.findall(r"<script[^>]*>(.{600,})</script>", text, re.I | re.S)
    for ch in chunks[:10]:
        if entropy(ch) >= 4.2:  # alto
            findings.append("entropy:high")
            break

    # Snippets: scripts/iframes y decodificados base64
    snippets = []
    for m in re.finditer(r"(?:<script[^>]*>.*?</script>)|(?:<iframe[^>]*>)", text, re.I | re.S):
        frag = text[m.start():m.end()]
        snippets.append(frag[:800])
        if len(snippets) >= 8:
            break

    for dec in decode_base64_chunks(text):
        snippets.append(dec[:800])
        if len(snippets) >= 12:
            break

    return findings, snippets

def scan_site_malware(start_url: str, max_pages=80, per_url_budget=0.2) -> Dict:
    """
    Crawler + DOM render + descubrimiento por sitemap y WP REST.
    Devuelve:
      {
        infected: bool,
        severity: "critical"|"high"|"medium"|"low"|None,
        urls: [ { url, findings_dom[], findings_raw[], external_scripts[], snippets[] }, ... ],
        summary: { suspected_urls[], counts: {...} }
      }
    """
    result = {"infected": False, "severity": None, "urls": [], "summary": {}}
    visited = set()

    # 0) Home + links
    home_html = _fetch(start_url)
    seed = [start_url] + collect_links(start_url, home_html, limit=min(40, max_pages//2))

    # 1) Sitemaps + REST Search
    more = []
    try:
        more = discover_urls(start_url, limit=500)
    except Exception:
        more = []
    # dedup y prioriza sospechosas
    all_urls = []
    seen = set()
    for u in seed + more:
        if u not in seen and same_host(start_url, u):
            seen.add(u); all_urls.append(u)
    def u_score(u: str):
        s = 0
        for kw in SUS_KEYWORDS:
            if re.search(kw, u, flags=re.I): s += 5
        return -s, len(u)
    all_urls = sorted(all_urls, key=u_score)[:max_pages]

    suspicious_hits = []
    counts = {"dom_hits":0, "raw_hits":0, "externals":0, "hidden":0, "keywords":0, "entropy":0, "meta":0}

    for u in all_urls:
        if u in visited: 
            continue
        visited.add(u)

        html_raw, html_dom, ext_js = render_dom(u)

        f_dom, sn_dom = find_indicators(html_dom)
        f_raw, _      = find_indicators(html_raw)

        entry = {
            "url": u,
            "findings_dom": f_dom,
            "findings_raw": f_raw,
            "external_scripts": ext_js[:80],
            "snippets": (sn_dom or [])[:6],
        }
        result["urls"].append(entry)

        # Contabiliza
        if f_dom: counts["dom_hits"] += 1
        if f_raw: counts["raw_hits"] += 1
        if any(x.startswith("externals:") for x in (f_dom+f_raw)): counts["externals"] += 1
        if any(x.startswith("hidden:") for x in (f_dom+f_raw)): counts["hidden"] += 1
        if any(x.startswith("keywords:") for x in (f_dom+f_raw)): counts["keywords"] += 1
        if any(x.startswith("entropy:") for x in (f_dom+f_raw)): counts["entropy"] += 1
        if any(x.startswith("meta:refresh") for x in (f_dom+f_raw)): counts["meta"] += 1

        if f_dom or f_raw:
            suspicious_hits.append(u)

        time.sleep(per_url_budget)

    result["summary"]["suspected_urls"] = suspicious_hits
    result["summary"]["counts"] = counts

    # Severidad
    score = 0
    score += 3 * min(counts["dom_hits"], 5)
    score += 2 * min(counts["raw_hits"], 5)
    score += 2 * counts["hidden"]
    score += 2 * counts["externals"]
    score += 1 * counts["keywords"]
    score += 2 * counts["entropy"]
    score += 2 * counts["meta"]
    # umbrales
    if score >= 12:
        sev = "critical"
    elif score >= 8:
        sev = "high"
    elif score >= 4:
        sev = "medium"
    elif score >= 2:
        sev = "low"
    else:
        sev = None

    result["infected"] = bool(sev)
    result["severity"] = sev
    return result
